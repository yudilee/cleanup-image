services:
  backend:
    build:
      context: ./backend
      args:
        # Change this to 'rocm' for AMD GPUs
        DEVICE_TYPE: ${DEVICE_TYPE:-cuda}
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
              # For AMD ROCm, Docker support is slightly different and often requires specific flags
              # or simply mapping /dev/kfd and /dev/dri.
              # If using ROCm, you might need to comment out the 'driver: nvidia' block
              # and use devices/volumes mapping like below:
              # devices:
              #   - /dev/kfd
              #   - /dev/dri
    volumes:
      - ./backend:/app
    environment:
      - PYTHONUNBUFFERED=1

  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:3000"
    environment:
      - API_URL=http://backend:8000
